# Build and Test

Last full review: 2025-12-29

There is a top-level Taskfile.yml that contains tasks for most of these things. Run `task -l`.

You can build and test for routine development with

```sh
./build_all
```

# Coverage
```sh
./coverage
open target/debug/coverage/index.html
```

Run
```sh
export LLVM_PROFILE_FILE="$PWD/.grcov/%p-%m.profraw"
```
to avoid creating profraw files when running things manually from a coverage build.

100% coverage is generally a non-goal, but it is expected in parts of the parser:
* common/parsing/pass1/
* common/parsing/pass2/
* In common/parsing/
  * layout.rs
  * model.rs
  * pass*.rs
  * score.rs
  * timeline.rs
* In common/parsing/score/
  * generator.rs

The rationale for 100% coverage is to ensure that all error conditions are tested and that there are no unreachable code paths. Unreachable code paths would indicate that the parser isn't coded as tightly as it should be. These would arise if later code relies on earlier validations, which is fragile. Requiring that 100% of the parser code is reachable helps to ensure that we have a tight grammar.

We do not rely *only* on coverage. There are a lot of logic that could be missed even with 100% coverage.

# Keyboard Testing

The engine and some other parts of the keyboard have unit tests. Most of the complex logic is tested that way, including some of the web logic. This includes complex keyboard interaction logic. For the most part, we rely on manual testing for the keyboards themselves.

On Linux, you can watch Syntoniq's MIDI output with `aseqdump`, e.g.:
```sh
aconnect -l
aseqdump -p 128:0
```

# Other Tools

* `tokenize` -- parses a score file into JSON and can handle all passes separately. This is the only way to see JSON for layouts and intermediate tokens.
* `common/parsing-tests/refresh` -- runs `tokenize` to create output files.

# Test Workflow

Run `./build_all`. Most tests are dealt with in the normal way. There are two cases where large files are generated and can't usually be generated manually:

## Parsing Tests

These are generated by `common/parsing-tests/refresh`. When iterating on files in `common/parsing-tests/` or when changing the parser, it is common to rerun that file and diff.

See `common/src/parsing/tests.rs` for emacs lisp code that helps with looking at files. It helps move span by span through the output and errors and contains additional information. The tests that have special workflows also output information when they fail. Comments in the test code itself provide additional information.

## Generator Tests

Score files in `syntoniq/test-data/*.stq` are converted to all supported output formats. If there are any mismatches, the mismatches are written to `syntoniq/test-data/actual/`. JSON and csound files can be directly diffed. Note that the csd files contain spans, so diffs show changes when there are no music changes. This is not true of MIDI. MIDI files can be diffed by using `midicsv` to generate CSV files first.

The best way to check generated output is to listen to them. Unless obvious, the stq files contain comments explaining what to listen for.

* Play a csound file with `csound file.csd`.
* FluidSynth can play MIDI files with MPE. (TiMidity++ cannot.): `fluidsynth -iq file.midi -F /tmp/a.wav` produces a WAV file.
* You can load MIDI files into a DAW, but the easiest way to send them through Surge XT:
  * Start surge XT or any other synth that listens to MIDI events
  * Use `aplaymidi -l` to find suitable port; on Linux, the MIDI through port is a good choice
  * `aplaymidi --port 'Midi Through' file.midi`
